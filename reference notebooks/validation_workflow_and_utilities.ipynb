{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Validation/Cross-Validation For Model Selection\n",
    "\n",
    "This notebook demonstrates two typical workflows for using validation data to select models. It also demonstrates the use of some utility methods like generating **polynomial features**  and **scaling features**.\n",
    "\n",
    "**Notebook Contents**\n",
    "\n",
    "> 1. Simple preprocessing\n",
    "> 2. Basic validation method: Train/validation/test\n",
    "> 3. Rigorous validation method: Cross-validation/test\n",
    "> 4. Making CV less manual via scikit-learn\n",
    "\n",
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading: cars data set (using car characteristics to predict the price)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data', header=None)\n",
    "\n",
    "columns= ['symboling','normalized-losses','make','fuel-type',\n",
    "          'aspiration','num-of-doors','body-style','drive-wheels',\n",
    "          'engine-location','wheel-base','length','width','height',\n",
    "          'curb-weight','engine-type','num-of-cylinders','engine-size',\n",
    "          'fuel-system','bore','stroke','compression-ratio','horsepower',\n",
    "          'peak-rpm','city-mpg','highway-mpg','price']\n",
    "df.columns=columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to simplify things a bit by focusing on the numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple cleaning\n",
    "df = df.replace('?', np.NaN).dropna().reset_index(drop=True)\n",
    "df['price'] = df['price'].astype(float)\n",
    "\n",
    "cars = df.select_dtypes(exclude=['object']).copy()\n",
    "\n",
    "# cars['make'] = df['make']\n",
    "cars.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start modeling! We're going to try out the validation process to choose between 3 models: simple linear regression, linear regression with 5th degree polynomial features, and linear regression with 2nd degree polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Validation Method: Train / Validation / Test\n",
    "\n",
    "Here we will break the data into 3 portions: 60% for training, 20% for validation (used to select the model), 20% for final testing evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "\n",
    "X, y = cars.drop('price',axis=1), cars['price']\n",
    "\n",
    "# hold out 20% of the data for final testing\n",
    "X_, X_test, y_, y_test = train_test_split(X, y, test_size=.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise**: using `train_test_split` and random state 30, further partition X, y into datasets X_train, y_train (60% of original) and X_val, y_val (20%).\n",
    "\n",
    "Hint: you will need to adjust the `test_size` parameter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=.25, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Scaled Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "\n",
    "#Feature scaling for train, val, and test\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_val_scaled = scaler.transform(X_val.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature transforms for train, val, and test so that we can run our poly model on each\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train.values)\n",
    "X_val_poly = poly.transform(X_val.values)\n",
    "X_test_poly = poly.transform(X_test.values)\n",
    "\n",
    "lm_poly = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train, validate, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate\n",
    "\n",
    "lm.fit(X_train, y_train)\n",
    "print(f'Linear Regression val R^2: {lm.score(X_val, y_val):.3f}')\n",
    "\n",
    "lm_poly.fit(X_train_poly, y_train)\n",
    "print(f'Degree 2 polynomial regression val R^2: {lm_poly.score(X_val_poly, y_val):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out that negative R^2, some severe overfitting! \n",
    "\n",
    "So having run this validation step, we see that the evidence points to simple linear regression being the best model. So our validation process lets us **select** that choice of model, and as our final step we retrain it on the entire chunk of train/val data and see how it does on test data:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_,y_)\n",
    "print(f'Linear Regression test R^2: {lm.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terrible!\n",
    "\n",
    "This is a pretty solid selection method, but we can make it even more rigorous using **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise**: Return to the beginning of this workflow (train-test split), and try changing the random state (e.g. to 11) before stepping through all the code blocks.\n",
    "\n",
    "What happened to the evaluation results, and how would you explain it? Is the evidence about which model we should select the same, or different? \n",
    "       \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rigorous Validation Method: Cross-Validation / Test\n",
    "\n",
    "Here we will break the data into 2 portions: 80% for a cross-validated training process, and 20% for final testing evaluation. \n",
    "\n",
    "Remember that the idea of CV is to make efficient use of the data available to us (using 80% instead of 60% above), while also performing multiple validation checks. For k-fold CV, we come up with k train/validation splits of the whole chunk of data, in such a way that **each observation is in the validation set exactly 1 time**. Here's a helpful diagram:\n",
    "\n",
    "![](images/cross_validation_diagram.png)\n",
    "\n",
    "As we loop through our CV folds, we will train and validate both models and collect the results to compare at the end. Note that we scale the training features within the CV loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X, y = cars.drop('price',axis=1), cars['price']\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=10) #hold out 20% of the data for final testing\n",
    "\n",
    "#this helps with the way kf will generate indices below\n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the CV\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state = 71)\n",
    "cv_lm_r2s = [] #collect the validation results\n",
    "\n",
    "for train_ind, val_ind in kf.split(X,y):\n",
    "    \n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind] \n",
    "    \n",
    "    #simple linear regression\n",
    "    lm = LinearRegression()\n",
    "\n",
    "    lm.fit(X_train, y_train)\n",
    "    cv_lm_r2s.append(round(lm.score(X_val, y_val), 3))\n",
    "\n",
    "print('Simple regression scores: ', cv_lm_r2s, '\\n')\n",
    "\n",
    "print(f'Simple mean cv r^2: {np.mean(cv_lm_r2s):.3f} +- {np.std(cv_lm_r2s):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state = 71)\n",
    "cv_lm_poly_r2s = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(X,y):    \n",
    "    #poly with degree 2\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    lm_poly = LinearRegression()\n",
    "    \n",
    "    lm_poly.fit(X_train_poly, y_train)\n",
    "    cv_lm_poly_r2s.append(round(lm_poly.score(X_val_poly, y_val), 3))\n",
    "    \n",
    "print('Poly scores: ', cv_lm_poly_r2s, '\\n')\n",
    "\n",
    "print(f'Poly mean cv r^2: {np.mean(cv_lm_poly_r2s):.3f} +- {np.std(cv_lm_poly_r2s):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise (time permitting)**: Modify the Cross-validation loop above so that we also fit degree 3 and 4 polynomial regression model and track their validation mean squared error instead of r^2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-fold, in a Less Manual Way with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-fold loop we created above required a chunk of code, but we usually expect sklearn to let us make everything much simpler. It turns out we can in this case too, at the expense of fine-grained control over exactly what happens within the k-fold loop. When we want to scale each training set within the k-fold loop or apply other transformations, the fine-grained control is nice, but often we can keep things simple and use the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lm = LinearRegression()\n",
    "\n",
    "cross_val_score(lm, X, y, # estimator, features, target\n",
    "                cv=5, # number of folds \n",
    "                scoring='r2') # scoring metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also recreate the exact partitioning we used for the manual version by passing a KFold object to `cross_val_score` and using the same random state -- note the results below are identical to the manual output above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state = 71)\n",
    "cross_val_score(lm, X, y, cv=kf, scoring='r2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
